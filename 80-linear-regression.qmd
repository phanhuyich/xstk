# Hồi quy

Giả sử
$(X_i, Y_i)_{i=1,\ldots,n}\iid \P_{(X,Y)}$
với mật độ xác suất $h(x,y),$
mật độ xác suất biên $h(x)$ và
mật độ xác suất có điều kiện $h(y|x).$

:::{#def-regression}
### Regression function
Hồi quy là kỳ vọng về $Y$
khi biết $X:$
$$
x\mapsto f(x) \is \E{Y|X=x} = \int y h(y|x) dy
.$$
:::

:::{#def-conditional-quantile}
### Conditional quantile
Cho $\alpha\in[0,1],$
phân vị $q_{\alpha}$ cho $Y$
khi biết $X=x:$
$$
x\mapsto q_{\alpha}(x) \textrm{ such that }
\int_{-\infty}^{q_{\alpha}(x)} h(y|x) dy \equiv 1-\alpha
.$$
Đặt $\alpha=1/2$ ta có conditional median.
:::

## Hồi quy tuyến tính
:::{#def-linear-regression}
### Theoretical linear regression
Giả sử $\V{X}>0.$
Đường
hồi quy tuyến tính áp $Y$ lên $X$ là
$y = a x + b$
với hệ số
$$
(a, b) \is
\argmin_{(\alpha,\beta)\in\R^2} \E{(Y-\alpha X-\beta)^2}.
$$
:::
:::{.remark}
Lấy đạo hàm từng phần và giải hệ phương trình ta có
$$
\begin{split}
a &= \frac{\Cov{X,Y}}{\V{X}},\\
b &= \E{Y} - a\E{X}.
\end{split}
$$
:::

:::{#def-regression-noise}
### Noise
Nhiễu
$$
\epsilon\is Y-(a X+b)
,$$
có
$\E{\epsilon}\equiv 0, \Cov{X,\epsilon}\equiv 0.$
:::

:::{#def-least-squares-estimator}
### Least squares estimator, LSE
Quan sát $(X_i, Y_i), i=1,\ldots,n.$ LSE là điểm cực tiểu của
tổng bình phương lỗi (sum of squared errors):
$$
(\hat{a}, \hat{b}) \is
\argmin_{(\alpha,\beta)\in\R^2} \sum_{i=1}^{n}{(Y_i-\alpha X_i-\beta)^2}
.$$
:::
:::{.remark}
Lấy đạo hàm từng phần và giải hệ phương trình ta có
$$
\begin{split}
\hat{a} &= \frac{\overline{XY}-\bar{X}\bar{Y}}{\overline{X^2}-\bar{X}^2},\\
\hat{b} &= \bar{Y} - \hat{a}\bar{X}.
\end{split}
$$
:::

## Hồi quy đa biến

:::{#def-multivariate-linear-regression}
### Multivariate regression

Quan sát các cặp biến $(\X_i, Y_i), i=1,\ldots,n.$

- $\X_i\in\R^p$: nhập liệu đa chiều (multivariate input, explanatory variables, covariates, independent variables)
- $Y_i\in\R$: kết quả (output, response, explained variable, dependent variable)

Giả sử tọa độ đầu tiên của các biến $\X_{i}$ đều là 1.
Tìm hệ số $\vec{a}\in\R^p$ để

$$Y_i = \X_i^T\vec{a} + \epsilon_i, i=1,\ldots,n
,$$

trong đó
$\{\epsilon_i\}_{i=1,\ldots,n}$ là nhiễu.
:::

:::{#def-lse-multivars}
### Least squares estimator, LSE in matrix form
Sắp xếp

$$
\begin{split}
\XX &\is
\begin{pmatrix}
\X_1^T\\
\vdots\\
\X_n^T\\
\end{pmatrix}
\in\R^{n\times p},\\
\vec{\epsilon} &\is (\epsilon_1,\ldots,\epsilon_n)^T\in\R^n,\\
\vec{Y} &\is (Y_1,\ldots,Y_n)^T \in \R^n,\\
\implies \vec{Y} &= \XX \vec{a} + \vec{\epsilon}.
\end{split}
$$

Ta có LSE
$$
$$

$$
\hat{\vec{a}} \is
\argmin_{\vec{a}\in\R^p} \|\vec{Y} - \XX \vec{a}\|_2^2
.$$
:::

:::{.remark}
Nếu $\XX$ full rank:
$$
\hat{\vec{a}} \equiv (\XX^T\XX)^{-1}\XX^T\vec{Y}.
$$

Đặt $P\is \XX(\XX^T\XX)^{-1}\XX^T$ ta có
$$
\XX\hat{\vec{a}} = P\vec{Y} \equiv P^2\vec{Y},
$$
là xạ ảnh của $\vec{Y}$ lên không gian tuyến tính dựng trên các cột của $\XX .$
:::
